ba <- balacc(obs = obs,pred = pred)
fpr <- FPR(obs = obs,pred = pred)
fnr <- FNR(obs = obs,pred = pred)
return(c(model_name,acc,f1,auc,errate,precision,recall,specificity,ba,fpr,fnr))
}
comparison_df = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(comparison_df) = columns
preds_knn
preds_qda
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", as.numeric(test_set$Classes), as.numeric(preds_logistic))
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", test_set$Classes, preds_knn)
preds_knn
test_set$Classes
as.numeric(preds_knn)
as.numeric(test_set$Classes)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", as.numeric(test_set$Classes), as.numeric(preds_knn))
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", as.numeric(test_set$Classes), as.numeric(preds_logistic))
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", as.numeric(test_set$Classes), as.numeric(preds_knn))
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
test_set$Classes
preds_sgb
preds_sgb = predict(SGB,test_set, type="raw")
confusionMatrix(preds_sgb, as.factor(test_set$Classes),
mode = "everything",
positive='1')
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", as.numeric(test_set$Classes), as.numeric(preds_logistic))
comparison_df[nrow(comparison_df) +
1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", as.numeric(test_set$Classes), as.numeric(preds_knn))
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM", test_set$Classes, preds_svm)
test_set$Classes
preds_svm
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM", as.numeric(test_set$Classes), as.numeric(preds_svm_test_linear))
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", as.numeric(test_set$Classes), as.numeric(preds_logistic))
comparison_df[nrow(comparison_df) +
1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", as.numeric(test_set$Classes), as.numeric(preds_knn))
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM", as.numeric(test_set$Classes), as.numeric(preds_svm_test_linear))
#Import metrica package that gives access to a variety of performance metrics in R
library(metrica)
#First we initialize a vector with our column names
columns = c("Model","Accuracy", "F1-Score", "AUC_roc","Error_Rate","Precision","Recall", "Specificity", "Balanced Accuracy", "FPR", "FNR")
#Now we define a function that takes in our model, reference which is our test set, predictions and the dataframe
comparison_metrics <- function(model_name, obs, pred) {
acc <- accuracy(obs = obs,pred = pred)
f1 <- fscore(obs = obs,pred = pred)
auc <- AUC_roc(obs = obs,pred = pred)
errate <- error_rate(obs = obs,pred = pred)
precision <- precision(obs = obs,pred = pred)
recall <- recall(obs = obs,pred = pred)
specificity <- specificity(obs = obs,pred = pred)
ba <- balacc(obs = obs,pred = pred)
fpr <- FPR(obs = obs,pred = pred)
fnr <- FNR(obs = obs,pred = pred)
return(c(model_name,acc,f1,auc,errate,precision,recall,specificity,ba,fpr,fnr))
}
comparison_df = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(comparison_df) = columns
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", as.numeric(test_set$Classes), as.numeric(preds_logistic))
comparison_df[nrow(comparison_df) +
1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", as.numeric(test_set$Classes), as.numeric(preds_knn))
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM", as.numeric(test_set$Classes), as.numeric(preds_svm_test_linear))
comparison_df
library(cvAUC)
install.packages("cvAUC")
library(cvAUC)
#Import metrica package that gives access to a variety of performance metrics in R
library(metrica)
library(cvAUC)
#First we initialize a vector with our column names
columns = c("Model","Accuracy", "F1-Score", "AUC_roc","Error_Rate","Precision","Recall", "Specificity", "Balanced Accuracy", "FPR", "FNR")
#Now we define a function that takes in our model, reference which is our test set, predictions and the dataframe
comparison_metrics <- function(model_name, obs, pred) {
acc <- accuracy(obs = obs,pred = pred)
f1 <- fscore(obs = obs,pred = pred)
auc <- AUC(pred, obs)
errate <- error_rate(obs = obs,pred = pred)
precision <- precision(obs = obs,pred = pred)
recall <- recall(obs = obs,pred = pred)
specificity <- specificity(obs = obs,pred = pred)
ba <- balacc(obs = obs,pred = pred)
fpr <- FPR(obs = obs,pred = pred)
fnr <- FNR(obs = obs,pred = pred)
return(c(model_name,acc,f1,auc,errate,precision,recall,specificity,ba,fpr,fnr))
}
comparison_df = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(comparison_df) = columns
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", as.numeric(test_set$Classes), as.numeric(preds_logistic))
comparison_df[nrow(comparison_df) +
1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
#Import metrica package that gives access to a variety of performance metrics in R
library(metrica)
library(cvAUC)
#First we initialize a vector with our column names
columns = c("Model","Accuracy", "F1-Score", "AUC_roc","Error_Rate","Precision","Recall", "Specificity", "Balanced Accuracy", "FPR", "FNR")
#Now we define a function that takes in our model, reference which is our test set, predictions and the dataframe
comparison_metrics <- function(model_name, obs, pred) {
obs = as.numeric(obs)
pred = as.numeric(pred)
acc <- accuracy(obs = obs,pred = pred)
f1 <- fscore(obs = obs,pred = pred)
auc <- AUC(pred, obs)
errate <- error_rate(obs = obs,pred = pred)
precision <- precision(obs = obs,pred = pred)
recall <- recall(obs = obs,pred = pred)
specificity <- specificity(obs = obs,pred = pred)
ba <- balacc(obs = obs,pred = pred)
fpr <- FPR(obs = obs,pred = pred)
fnr <- FNR(obs = obs,pred = pred)
return(c(model_name,acc,f1,auc,errate,precision,recall,specificity,ba,fpr,fnr))
}
comparison_df = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(comparison_df) = columns
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", as.numeric(test_set$Classes), as.numeric(preds_logistic))
comparison_df[nrow(comparison_df) +
1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", test_set$Classes,preds_knn)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM", test_set$Classes, preds_svm_test_linear)
comparison_df
auc
comparison_df
comparison_df_sorted <- comparison_df.sort_values(by = "AUC")
comparison_df_sorted <- comparison_df[order(AUC),]
comparison_df_sorted <- comparison_df[order(AUC_roc),]
comparison_df_sorted <- comparison_df[order(AUC_roc),]
comparison_df_sorted <- comparison_df[order(c(AUC_roc)),]
comparison_df_sorted
comparison_df_sorted <- comparison_df[order(c(AUC_roc),),]
comparison_df_sorted <- comparison_df[order("AUC-roc"),]
comparison_df_sorted <- comparison_df[order("AUC-roc"),]
comparison_df_sorted
comparison_df_sorted <- comparison_df[order(AUC_roc()),]
comparison_df_sorted <- comparison_df[order(AUC_roc,]
comparison_df
comparison_df_sorted <- comparison_df[order(AUC_roc),]
comparison_df_sorted <- comparison_df[AUC_roc,]
comparison_df_sorted <- comparison_df[AUC_roc]
comparison_df_sorted <- comparison_df["AUC_roc"]
comparison_df_sorted
comparison_df_sorted <- comparison_df[order("AUC_roc")]
comparison_df_sorted <- comparison_df[order(comparison_df$AUC_roc)]
comparison_df_sorted
comparison_df_sorted <- comparison_df[order(comparison_df$AUC_roc),]
comparison_df_sorted
comparison_df_sorted <- comparison_df[order(c-omparison_df$AUC_roc),]
comparison_df_sorted <- comparison_df[order(-comparison_df$AUC_roc),]
comparison_df_sorted
barplot(comparison_df_sorted, main ="AUC Comparison", xlab="AUC_roc")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=Accuracy, fill=Accuracy)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=AUC_roc, fill=Accuracy)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
ggplot(aes(x=Model_Name, y=AUC_Values, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
summarise(AUC_Values = AUC_roc, Model_Name = Model)+
ggplot(aes(x=Model_Name, y=AUC_Values, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
group_by(Model) +
summarise(AUC_Values = AUC_roc, Model_Name = Model)+
ggplot(aes(x=Model_Name, y=AUC_Values, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
group_by(Model) %>%
summarise(AUC_Values = AUC_roc, Model_Name = Model) %>%
ggplot(aes(x=Model_Name, y=AUC_Values, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
summarise(AUC_Values = AUC_roc, Model_Name = Model) %>%
ggplot(aes(x=Model_Name, y=AUC_Values, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
order_by(Accuracy) %>%
ggplot(aes(x=Model, y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
arrange(Accuracy) %>%
ggplot(aes(x=Model, y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
arrange(Accuracy) %>%
mutate(Model = factor(model, levels=Model)) %>%
ggplot(aes(x=Model, y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=reorder(AUC_roc), fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=reorder(AUC_roc),AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=reorder(AUC_roc),AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=reorder(AUC_roc),AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=Model, y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
omparison_df_sorted
comparison_df_sorted
comparison_df_sorted %>%
ggplot(aes(x=Model, y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=(Model,+AUC_roc), y=AUC_roc, fill=AUC_roc)) +
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")+
scale_fill_manual(c('blue','red'))
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=AUC_roc)) +
geom_col(position="dodge")+
scale_fill_manual(values = c('blue','red'))
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
geom_col(position="dodge")
comparison_df_sorted %>%
summarise(Model_Name = reorder(Model,+AUC_roc), AUC_Value = AUC_roc)+
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
geom_col(position="dodge")
AUC_roc
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank(), axis.text.x = "Models") +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank(), axis.text.x =Models) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank(), axis.title.x ="Models") +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank(), axis.title.x =Models) +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models") +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models Tested", y = "AUC Value") +
geom_col(position="dodge")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models Tested", y  "AUC Value") +
labs(x = "Models Tested", y = "AUC V
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models Tested", y = "AUC Value") +
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models Tested", y = "AUC Value") +
geom_col(position="dodge")
#Import metrica package that gives access to a variety of performance metrics in R
library(metrica)
library(cvAUC)
#First we initialize a vector with our column names
columns = c("Model","Accuracy", "F1-Score", "AUC_roc","Error_Rate","Precision","Recall", "Specificity", "Balanced Accuracy", "FPR", "FNR")
#Now we define a function that takes in our model, reference which is our test set, predictions and the dataframe
comparison_metrics <- function(model_name, obs, pred) {
obs = as.numeric(obs)
pred = as.numeric(pred)
acc <- accuracy(obs = obs,pred = pred)
f1 <- fscore(obs = obs,pred = pred)
auc <- AUC(pred, obs)
errate <- error_rate(obs = obs,pred = pred)
precision <- precision(obs = obs,pred = pred)
recall <- recall(obs = obs,pred = pred)
specificity <- specificity(obs = obs,pred = pred)
ba <- balacc(obs = obs,pred = pred)
fpr <- FPR(obs = obs,pred = pred)
fnr <- FNR(obs = obs,pred = pred)
return(c(model_name,acc,f1,auc,errate,precision,recall,specificity,ba,fpr,fnr))
}
comparison_df = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(comparison_df) = columns
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", test_set$Classes, preds_logistic)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", test_set$Classes,preds_knn)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM", test_set$Classes, preds_svm_test_linear)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM", test_set$Classes, preds_svm_test_radial)
comparison_df
#Import metrica package that gives access to a variety of performance metrics in R
library(metrica)
library(cvAUC)
#First we initialize a vector with our column names
columns = c("Model","Accuracy", "F1-Score", "AUC_roc","Error_Rate","Precision","Recall", "Specificity", "Balanced Accuracy", "FPR", "FNR")
#Now we define a function that takes in our model, reference which is our test set, predictions and the dataframe
comparison_metrics <- function(model_name, obs, pred) {
obs = as.numeric(obs)
pred = as.numeric(pred)
acc <- accuracy(obs = obs,pred = pred)
f1 <- fscore(obs = obs,pred = pred)
auc <- AUC(pred, obs)
errate <- error_rate(obs = obs,pred = pred)
precision <- precision(obs = obs,pred = pred)
recall <- recall(obs = obs,pred = pred)
specificity <- specificity(obs = obs,pred = pred)
ba <- balacc(obs = obs,pred = pred)
fpr <- FPR(obs = obs,pred = pred)
fnr <- FNR(obs = obs,pred = pred)
return(c(model_name,acc,f1,auc,errate,precision,recall,specificity,ba,fpr,fnr))
}
comparison_df = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(comparison_df) = columns
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", test_set$Classes, preds_logistic)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", test_set$Classes,preds_knn)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM_linear", test_set$Classes, preds_svm_test_linear)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM_radial", test_set$Classes, preds_svm_test_radial)
comparison_df
comparison_df_sorted <- comparison_df[order(-comparison_df$AUC_roc),]
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models Tested", y = "AUC Value") +
geom_col(position="dodge")
df_b[["Region"]] = 0
df_s[["Region"]] = 1
dim(df)
str(df)
summary(df)
unique(df$year)
unique(df$month)
colSums(is.na(df))
df = df %>% drop_na(DC)
dim(df)
colSums(is.na(df))
df = df %>% drop_na(DC)
dim(df)
unique(df$Classes)
df$Classes <- as.numeric(df$Classes)
str(df)
aggregate(df$Classes ~ df$Region, FUN = sum)
aggregate(df$Temperature ~ df$Region, FUN = mean)
df %>%
group_by(Region) %>%
summarise(Region = Region, Number_of_fires = sum(Classes), Temperature = mean(Temperature)) %>%
ggplot(aes(x=Region, y=Number_of_fires, fill = Temperature))+
geom_col(position='dodge')
corr_mat <- round(cor(df_scaled),2)
p_mat <- cor_pmat(df_scaled)
corr_mat <- ggcorrplot(
corr_mat,
hc.order = FALSE,
type = "upper",
outline.col = "white",
)
ggplotly(corr_mat)
confusionMatrix(preds_pruned, train_set$Classes,
mode = "everything",
positive='1')
pruned_base_model
preds_pruned
train_set$Classes
preds_pruned = predict(pruned_base_model,train_set, type="class")
confusionMatrix(preds_pruned, train_set$Classes,
mode = "everything",
positive='1')
confusionMatrix(preds_pruned, train_set$Classes,
mode = "everything",
positive='1')
df_b <- read.csv("./Algerian_forest_fires_dataset_Bejaia.csv")
df_s <- read.csv("./Algerian_forest_fires_dataset_Sidi_Bel_Abbes.csv")
pred <- prediction(as.numeric(preds), test_set$Classes)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
pred <- prediction(as.numeric(preds), as.numeric(test_set$Classes))
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
auc
auc
preds_pruned = predict(pruned_base_model,test_set, type="class")
confusionMatrix(preds_pruned, test_set$Classes,
mode = "everything",
positive='1')
pred <- prediction(preds, test_set$Classes)
pred <- prediction(as.numeric(preds_pruned), test_set$Classes)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
# prepare training scheme
df_scaled$Classes = as.factor(df_scaled$Classes)
# prepare training scheme
df_scaled$Classes = as.factor(df_scaled$Classes)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
modelLDA <- train(Classes~., data=df_scaled, method="stepLDA", trControl=control)
importanceLDA <- varImp(modelLDA, scale=FALSE)
plot(importanceLDA)
ROCPred <- prediction(as.numeric(preds_qda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(ROCPer)
pred <- prediction(as.numeric(preds_pruned), test_set$Classes)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
perf <- performance(pred,"tpr","fpr")
plot(perf)
ROCPred <- prediction(as.numeric(preds_svm_test_linear), as.numeric(test_set_svm$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(ROCPer)
ROCPred <- prediction(as.numeric(preds_svm_test_linear), as.numeric(test_set_svm$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(ROCPer, print="auc")
ROCPred <- prediction(as.numeric(preds_lda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(ROCPer)
ROCPred <- prediction(as.numeric(preds_lda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
ROCPred <- prediction(as.numeric(preds_lda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
