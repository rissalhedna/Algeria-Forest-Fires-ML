preds_knn = predict(knn_cv,test_set_knn, type="raw")
confusionMatrix(preds_knn, test_set_knn$Classes,
mode = "everything",
positive="fire")
plot(perf)
ROCPred <- prediction(as.numeric(preds_qda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
preds_knn = predict(knn_cv,train_set_knn, type = "prob")
train_set %>%
ggplot() +
aes(x = preds_knn$fire, fill = Classes) +
geom_histogram(bins = 20) +
labs(x = "Probability", y = "Count", title = "Distribution of predicted probabilities for value fire" )
preds_knn = predict(knn_cv,test_set_knn, type="raw")
confusionMatrix(preds_knn, test_set_knn$Classes,
mode = "everything",
positive="fire")
preds <- prediction(as.numeric(preds_knn), as.numeric(test_set_knn$Classes))
perf <- performance(preds,"tpr","fpr")
auc <- performance(preds, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(perf)
base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class")
summary(base_model)
#Plot Decision Tree
rpart.plot(base_model)
preds_unpruned= predict(base_model, train_set, type="class")
confusionMatrix(preds_unpruned, train_set$Classes, mode = "everything", positive='1')
preds_unpruned= predict(base_model, train_set, type="class")
confusionMatrix(preds_unpruned, train_set$Classes, mode = "everything", positive='1')
preds_unpruned = predict(base_model,test_set, type="class")
confusionMatrix(preds_unpruned, test_set$Classes,
mode = "everything",
positive='1')
preds <- prediction(as.numeric(preds_unpruned), test_set$Classes)
perf <- performance(preds,"tpr","fpr")
auc <- performance(preds, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(perf)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 8))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
preds_pruned = predict(pruned_base_model,train_set, type="class")
confusionMatrix(preds_pruned, train_set$Classes,
mode = "everything",
positive='1')
preds_pruned = predict(pruned_base_model,test_set, type="class")
confusionMatrix(preds_pruned, test_set$Classes,
mode = "everything",
positive='1')
preds_pruned = predict(pruned_base_model,train_set, type="class")
confusionMatrix(preds_pruned, train_set$Classes,
mode = "everything",
positive='1')
preds_pruned = predict(pruned_base_model,test_set, type="class")
confusionMatrix(preds_pruned, test_set$Classes,
mode = "everything",
positive='1')
pred <- prediction(as.numeric(preds_pruned), test_set$Classes)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
perf <- performance(pred,"tpr","fpr")
plot(perf)
control <- trainControl(method="repeatedcv", number=5, repeats=5)
bagCART_model <- train(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set, method="treebag", metric="Accuracy", trControl=control)
preds_bag = predict(bagCART_model,train_set, type="raw")
confusionMatrix(preds_bag, train_set$Classes,
mode = "everything",
positive='1')
preds_bag = predict(bagCART_model,test_set, type="raw")
confusionMatrix(preds_bag, as.factor(test_set$Classes),
mode = "everything",
positive='1')
set.seed(40)
split <- sample.split(df_scaled, SplitRatio=0.8)
train_set <- subset(df_scaled, split == "TRUE")
test_set <- subset(df_scaled, split=="FALSE")
dim(train_set)
dim(test_set)
logistic_model <- glm(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set, family="binomial")
logistic_model
preds_logistic <- predict(logistic_model, train_set, type="response")
preds_logistic <- ifelse(preds_logistic >0.5,1,0)
preds_logistic <- as.factor(preds_logistic)
confusionMatrix(preds_logistic, train_set$Classes,
mode = "everything",
positive="1")
train_set$Classes <- as.factor(train_set$Classes)
test_set$Classes <- as.factor(test_set$Classes)
preds_logistic <- predict(logistic_model, test_set, type="response")
preds_logistic <- ifelse(preds_logistic >0.5,1,0)
preds_logistic <- as.factor(preds_logistic)
confusionMatrix(preds_logistic, test_set$Classes,
mode = "everything",
positive="1")
preds_logistic <- predict(logistic_model, train_set, type="response")
preds_logistic <- ifelse(preds_logistic >0.5,1,0)
preds_logistic <- as.factor(preds_logistic)
confusionMatrix(preds_logistic, train_set$Classes,
mode = "everything",
positive="1")
train_set$Classes <- as.factor(train_set$Classes)
test_set$Classes <- as.factor(test_set$Classes)
preds_logistic <- predict(logistic_model, test_set, type="response")
preds_logistic <- ifelse(preds_logistic >0.5,1,0)
preds_logistic <- as.factor(preds_logistic)
preds_logistic <- predict(logistic_model, train_set, type="response")
preds_logistic <- ifelse(preds_logistic >0.5,1,0)
preds_logistic <- as.factor(preds_logistic)
confusionMatrix(preds_logistic, train_set$Classes,
mode = "everything",
positive="1")
preds_logistic <- predict(logistic_model, test_set, type="response")
preds_logistic <- ifelse(preds_logistic >0.5,1,0)
preds_logistic <- as.factor(preds_logistic)
confusionMatrix(preds_logistic, test_set$Classes,
mode = "everything",
positive="1")
ROCPred <- prediction(as.numeric(preds_logistic), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
print(paste("The AUC is: ",auc))
lda_model = lda(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set, family="binomial")
lda_model
preds_lda = predict(lda_model,train_set, type="response")
confusionMatrix(preds_lda$class, train_set$Classes,
mode = "everything",
positive="1")
preds_lda = predict(lda_model,test_set, type="response")
confusionMatrix(preds_lda$class, test_set$Classes,
mode = "everything",
positive="1")
ROCPred <- prediction(as.numeric(preds_lda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
print(paste("The AUC is: ",auc))
qda_model = qda(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set)
qda_model
preds_qda = predict(qda_model,train_set, type="response")
confusionMatrix(preds_qda$class, train_set$Classes,
mode = "everything",
positive="1")
preds_qda = predict(qda_model,test_set, type="response")
confusionMatrix(preds_qda$class, test_set$Classes,
mode = "everything",
positive="1")
ROCPred <- prediction(as.numeric(preds_qda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
training_control <- trainControl(method = "repeatedcv",
summaryFunction = defaultSummary,
classProbs = TRUE,
number = 10,
repeats = 10)
train_set_knn <- train_set
test_set_knn <- test_set
train_set_knn$Classes <- mapvalues(train_set$Classes, from=c(0,1), to=c("not_fire","fire"))
test_set_knn$Classes <- mapvalues(test_set$Classes, from=c(0,1), to=c("not_fire","fire"))
knn_cv <- train(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH,
data = train_set_knn,
method = "knn",
trControl = training_control,
metric = "Accuracy",
tuneGrid = data.frame(k = seq(3, 85, by = 2)))
preds_knn = predict(knn_cv,test_set_knn, type="raw")
confusionMatrix(preds_knn, test_set_knn$Classes,
mode = "everything",
positive="fire")
preds <- prediction(as.numeric(preds_knn), as.numeric(test_set_knn$Classes))
perf <- performance(preds,"tpr","fpr")
auc <- performance(preds, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(perf)
library(rpart)
library(rpart.plot)
base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class")
summary(base_model)
#Plot Decision Tree
rpart.plot(base_model)
preds_unpruned= predict(base_model, train_set, type="class")
confusionMatrix(preds_unpruned, train_set$Classes, mode = "everything", positive='1')
preds_unpruned = predict(base_model,test_set, type="class")
confusionMatrix(preds_unpruned, test_set$Classes,
mode = "everything",
positive='1')
preds <- prediction(as.numeric(preds_unpruned), test_set$Classes)
perf <- performance(preds,"tpr","fpr")
auc <- performance(preds, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(perf)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 8))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
preds_pruned = predict(pruned_base_model,train_set, type="class")
confusionMatrix(preds_pruned, train_set$Classes,
mode = "everything",
positive='1')
preds_pruned = predict(pruned_base_model,test_set, type="class")
confusionMatrix(preds_pruned, test_set$Classes,
mode = "everything",
positive='1')
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 8))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class")
library(rpart)
library(rpart.plot)
base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class")
summary(base_model)
#Plot Decision Tree
rpart.plot(base_model)
preds_unpruned= predict(base_model, train_set, type="class")
confusionMatrix(preds_unpruned, train_set$Classes, mode = "everything", positive='1')
preds_unpruned = predict(base_model,test_set, type="class")
confusionMatrix(preds_unpruned, test_set$Classes,
mode = "everything",
positive='1')
preds <- prediction(as.numeric(preds_unpruned), test_set$Classes)
perf <- performance(preds,"tpr","fpr")
auc <- performance(preds, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(perf)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 8))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 3))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 0))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 8))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 4, minsplit = 8))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 8))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 3))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 10))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
pruned_base_model <- rpart(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set, method = "class",  control = rpart.control(cp = 0, maxdepth = 8, minsplit = 5))
summary(pruned_base_model)
#Plot Decision Tree
printcp(pruned_base_model)
rpart.plot(pruned_base_model)
preds_pruned = predict(pruned_base_model,train_set, type="class")
confusionMatrix(preds_pruned, train_set$Classes,
mode = "everything",
positive='1')
preds_pruned = predict(pruned_base_model,test_set, type="class")
confusionMatrix(preds_pruned, test_set$Classes,
mode = "everything",
positive='1')
pred <- prediction(as.numeric(preds_pruned), test_set$Classes)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
perf <- performance(pred,"tpr","fpr")
plot(perf)
print(paste("The AUC is: ",auc))
control <- trainControl(method="repeatedcv", number=5, repeats=5)
bagCART_model <- train(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set, method="treebag", metric="Accuracy", trControl=control)
preds_bag = predict(bagCART_model,train_set, type="raw")
confusionMatrix(preds_bag, train_set$Classes,
mode = "everything",
positive='1')
preds_bag = predict(bagCART_model,test_set, type="raw")
confusionMatrix(preds_bag, as.factor(test_set$Classes),
mode = "everything",
positive='1')
control <- trainControl(method="repeatedcv", number=5, repeats=5)
rf_model <- train(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set, method="rf", metric="Accuracy", trControl=control)
preds_rf = predict(rf_model,train_set, type="raw")
confusionMatrix(preds_rf, train_set$Classes,
mode = "everything",
positive='1')
preds_rf = predict(rf_model,test_set, type="raw")
confusionMatrix(preds_rf, as.factor(test_set$Classes),
mode = "everything",
positive='1')
control <- trainControl(method="cv", number=5)
SGB <- train(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set, method="gbm", metric="Accuracy", trControl=control)
preds_sgb = predict(SGB,train_set, type="raw")
confusionMatrix(preds_sgb, train_set$Classes,
mode = "everything",
positive='1')
preds_sgb = predict(SGB,test_set, type="raw")
confusionMatrix(preds_sgb, as.factor(test_set$Classes),
mode = "everything",
positive='1')
train_set_svm <- train_set
test_set_svm <- test_set
train_set_svm$Classes <- mapvalues(train_set$Classes, from=c(0,1), to=c("not_fire","fire"))
test_set_svm$Classes <- mapvalues(test_set$Classes, from=c(0,1), to=c("not_fire","fire"))
control = trainControl(method = "repeatedcv", repeats=5, summaryFunction=twoClassSummary, classProbs=TRUE)
svm_model_radial <- train(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set_svm, method="svmRadial", tuneLength=10, metric="ROC", trControl=control)
svm_model_radial <- train(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set_svm, method="svmRadial", tuneLength=10, metric="ROC", trControl=control)
svm_model_linear <- train(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data = train_set_svm, method="svmLinear", tuneLength=10, metric="ROC", trControl=control)
svm_model_radial
svm_model_linear
preds_svm_train_linear <- predict(svm_model_linear, train_set_svm)
preds_svm_train_radial <- predict(svm_model_radial, train_set_svm)
confusionMatrix(preds_svm_train_linear,train_set_svm$Classes, positive="fire")
confusionMatrix(preds_svm_train_radial,train_set_svm$Classes, positive="fire")
preds_svm_test_linear <- predict(svm_model_linear, test_set_svm)
preds_svm_test_radial <- predict(svm_model_radial, test_set_svm)
confusionMatrix(preds_svm_test_linear,test_set_svm$Classes, positive="fire")
confusionMatrix(preds_svm_test_radial,test_set_svm$Classes, positive="fire")
ROCPred <- prediction(as.numeric(preds_svm_test_linear), as.numeric(test_set_svm$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
ROCPred <- prediction(as.numeric(preds_svm_test_linear), as.numeric(test_set_svm$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
ROCPred <- prediction(as.numeric(preds_svm_test_radial), as.numeric(test_set_svm$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
print(paste("The AUC is: ",auc))
#Import metrica package that gives access to a variety of performance metrics in R
library(metrica)
library(cvAUC)
#First we initialize a vector with our column names
columns = c("Model","Accuracy", "F1-Score", "AUC_roc","Error_Rate","Precision","Recall", "Specificity", "Balanced Accuracy", "FPR", "FNR")
#Now we define a function that takes in our model, reference which is our test set, predictions and the dataframe
comparison_metrics <- function(model_name, obs, pred) {
obs = as.numeric(obs)
pred = as.numeric(pred)
acc <- accuracy(obs = obs,pred = pred)
f1 <- fscore(obs = obs,pred = pred)
auc <- AUC(pred, obs)
errate <- error_rate(obs = obs,pred = pred)
precision <- precision(obs = obs,pred = pred)
recall <- recall(obs = obs,pred = pred)
specificity <- specificity(obs = obs,pred = pred)
ba <- balacc(obs = obs,pred = pred)
fpr <- FPR(obs = obs,pred = pred)
fnr <- FNR(obs = obs,pred = pred)
return(c(model_name,acc,f1,auc,errate,precision,recall,specificity,ba,fpr,fnr))
}
colnames(comparison_df) = columns
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", test_set$Classes, preds_logistic)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", test_set$Classes,preds_knn)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM_linear", test_set$Classes, preds_svm_test_linear)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM_radial", test_set$Classes, preds_svm_test_radial)
comparison_df
#Import metrica package that gives access to a variety of performance metrics in R
library(metrica)
library(cvAUC)
#First we initialize a vector with our column names
columns = c("Model","Accuracy", "F1-Score", "AUC_roc","Error_Rate","Precision","Recall", "Specificity", "Balanced Accuracy", "FPR", "FNR")
#Now we define a function that takes in our model, reference which is our test set, predictions and the dataframe
comparison_metrics <- function(model_name, obs, pred) {
obs = as.numeric(obs)
pred = as.numeric(pred)
acc <- accuracy(obs = obs,pred = pred)
f1 <- fscore(obs = obs,pred = pred)
auc <- AUC(pred, obs)
errate <- error_rate(obs = obs,pred = pred)
precision <- precision(obs = obs,pred = pred)
recall <- recall(obs = obs,pred = pred)
specificity <- specificity(obs = obs,pred = pred)
ba <- balacc(obs = obs,pred = pred)
fpr <- FPR(obs = obs,pred = pred)
fnr <- FNR(obs = obs,pred = pred)
return(c(model_name,acc,f1,auc,errate,precision,recall,specificity,ba,fpr,fnr))
}
comparison_df = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(comparison_df) = columns
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Logistic regression", test_set$Classes, preds_logistic)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("LDA", test_set$Classes, preds_lda$class )
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("QDA", test_set$Classes, preds_qda$class)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("KNN", test_set$Classes,preds_knn)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Unpruned_Trees", test_set$Classes, preds_unpruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Pruned_Trees", test_set$Classes, preds_pruned)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Bagged_Trees", test_set$Classes, preds_bag)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Random_Forest", test_set$Classes, preds_rf)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("Boosted_Trees", test_set$Classes, preds_sgb)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM_linear", test_set$Classes, preds_svm_test_linear)
comparison_df[nrow(comparison_df) + 1, ] <- comparison_metrics("SVM_radial", test_set$Classes, preds_svm_test_radial)
comparison_df
comparison_df_sorted <- comparison_df[order(-comparison_df$AUC_roc),]
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models Tested", y = "AUC Value") +
geom_col(position="dodge")
ROCPred <- prediction(as.numeric(preds_sgb), as.numeric(test_set_svm$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
comparison_df_sorted <- comparison_df[order(-comparison_df$Accuracy),]
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models Tested", y = "AUC Value") +
geom_col(position="dodge")
preds_qda = predict(qda_model,train_set, type="response")
confusionMatrix(preds_qda$class, train_set$Classes,
mode = "everything",
positive="1")
preds_qda = predict(qda_model,test_set, type="response")
confusionMatrix(preds_qda$class, test_set$Classes,
mode = "everything",
positive="1")
comparison_df_sorted %>%
ggplot(aes(x=reorder(Model,+AUC_roc), y=AUC_roc, fill=Model)) +
theme(axis.text.x=element_blank()) +
labs(x = "Models Tested", y = "Accuracy") +
geom_col(position="dodge")
ROCPred <- prediction(as.numeric(preds_sgb), as.numeric(test_set_svm$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
ROCPred <- prediction(as.numeric(preds_sgb), as.numeric(test_set$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer)
plot(ROCPer, add=TRUE)
ROCPred <- prediction(as.numeric(preds_sgb), as.numeric(test_set$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
print(paste("The AUC is: ",auc))
plot(ROCPer, add=TRUE)
pred <- prediction(as.numeric(preds_pruned), test_set$Classes)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
perf <- performance(pred,"tpr","fpr")
plot(perf)
perf <- performance(pred,"tpr","fpr")
print(paste("The AUC is: ",auc))
ROCPred <- prediction(as.numeric(preds_sgb), as.numeric(test_set$Classes))
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
print(paste("The AUC is: ",auc))
plot(ROCPer, add=TRUE)
plot(ROCPer)
