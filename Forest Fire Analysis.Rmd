---
title: <center>Algerian Forest Fire Analysis</center><br/>
output: html_notebook
author:
  - Mohamed Rissal Hedna 201906233
  - Younes Djemmal 201906303
---
## Overview
### Background

During the summer of 2012, [wild fires](https://www.un-spider.org/news-and-events/news/algeria-maps-summer-2012-wildfires-available) ravaged throughout the Algerian territory covering most of the northern part, especially the coastal cities. This disaster was due to the higher than average temperatures which reached as high as 50 degrees Celcius.

### Objectives 

One important measure against the reproduction of such disasters is the ability to predict their occurrence. Moreover, in this project, we will attempt to predict these forest fires based on multiple features related to weather indices.

### Dataset Description

The Dataset we will use to train and test our models consists of 244 observations on two Algerian Wilayas (cities): Sidi-Bel Abbes and Bejaia. The observations have been gathered throughout the duration of 4 months from June to September 2012 for both cities.

**The Dataset contains the following variables:**

1. Date: (DD/MM/YYYY) Day, month ('june' to 'september'), year (2012)
2. Temp: temperature noon (temperature max) in Celsius degrees: 22 to 42
3. RH: Relative Humidity in %: 21 to 90
4. Ws: Wind speed in km/h: 6 to 29
5. Rain: total day in mm: 0 to 16.8<br>
**FWI Components (check this [LINK](https://cwfis.cfs.nrcan.gc.ca/background/summary/fwi) for more information)**
6. Fine Fuel Moisture Code (FFMC) index from the FWI system: 28.6 to 92.5
7. Duff Moisture Code (DMC) index from the FWI system: 1.1 to 65.9
8. Drought Code (DC) index from the FWI system: 7 to 220.4
9. Initial Spread Index (ISI) index from the FWI system: 0 to 18.5
10. Build-up Index (BUI) index from the FWI system: 1.1 to 68
11. Fire Weather Index (FWI) Index: 0 to 31.1
12. Classes: two classes, namely "fire" and "not fire"

## Exploratory Data Analysis

We first start off by importing the necessary libraries for our analysis.

[INSERT DESCRIPTION OF EACH LIBRARY]

```{r, include=FALSE}

library(MASS)
library(dplyr)
library(vtable)
library(plyr)
library(ggplot2)
library(ggcorrplot)
library(plotly)
library(tidyverse)
#Feature selection libraries
library(mlbench)
library(caret)
#For Logistic regression
library(caTools)
#For ROC curve
library(ROCR)
```


### Importing the data 

The Dataset provided to us was in the form of a .csv file that contained two tables, one table for the observations belonging to the Sidi-Bel Abbes region, and the other for Bejaia. 

Before starting our analysis we separated the tables into two distinct files according to the region. We named both files *Algerian_forest_fires_dataset_Bejaia.csv* and *Algerian_forest_fires_dataset_Sidi_Bel_Abbes.csv* for Bejaia and Sidi-Bel Abbes respectively.


```{r, echo=FALSE}
df_b <- read.csv("./Algerian_forest_fires_dataset_Bejaia.csv")
df_s <- read.csv("./Algerian_forest_fires_dataset_Sidi_Bel_Abbes.csv")
```

### Cleaning and processing the data

We first check the existence of null values in the Dataset, none were found.


```{r}
colSums(is.na(df_b))
colSums(is.na(df_s))
```

We then process to add a column in both datasets to indicate the region(Wilaya) in each table. We chose the following encoding:

1. Bejaia = 0
2. Sidi-Bel Abbes = 1


```{r}
df_b[["Region"]] = 0
df_s[["Region"]] = 1
```

After that, we proceed to merge both our datasets into one single dataframe using *full_join()*, this will allow us to easily explore and analyze the data.

```{r}
df_s$DC <- as.double(df_s$DC)
df_s$FWI <- as.double(df_s$FWI)

df = full_join(df_s, df_b)

dim(df)
str(df)
```

```{r}
summary(df)
unique(df$year)
unique(df$month)
```

We check again for any *NA* values that might have been introduced into the dataset by merging the data from both tables, we found out there was one row that contained NA value in DC and FWI. We delete that row since it will not affect our overall dataset.

```{r}
colSums(is.na(df))
df = df %>% drop_na(DC)
dim(df)
```

We now proceed to display the different range of values some categorical variables might contain, mainly the Classes and the Region columns.


```{r}
unique(df$Classes)
unique(df$Region)
```

We find that the Classes column has values that contain unneeded space characters, we proceed to trim those spaces.

```{r}
df$Classes <- trimws(df$Classes, which = c("both"))
```



```{r}
unique(df$Classes)
df = df %>% drop_na(Classes)
```

```{r}
df$Classes <- mapvalues(df$Classes, from=c("not fire","fire"), to=c(0,1))
```

```{r}
unique(df$Classes)
df$Classes <- as.numeric(df$Classes)
st(df)
```

```{r}
df <- df[-c(3)]

df_scaled = df
df_scaled[-c(1,2,13,14)] <- scale(df[-c(1,2,13,14)])
st(df_scaled)

```

### Visualizing the data

We have ended up with a clean and scaled dataframe named *df_scaled*, which we will use to visualize and further explore our data.

Our first instinct is to compare the two regions together in terms of number of fires, and average temperature. 

```{r}
aggregate(df$Classes ~ df$Region, FUN = sum)
aggregate(df$Temperature ~ df$Region, FUN = mean)
```
We used the unscaled dataset to plot the real life values of the temperatures.

```{r}
df %>%
  group_by(Region) %>%
  summarise(Region = Region, Number_of_fires = sum(Classes), Temperature = mean(Temperature)) %>%
  ggplot(aes(x=Region, y=Number_of_fires, fill = Temperature))+
  geom_col(position='dodge')
```

We can see that the the Sidi-Bel Abbes region has in total a greater number of fires and a higher average temperature throughout the summer of 2012.

## Further Analysis
### Correlation Matrix

The previous results push us to suspect a positive relationship between the temperature and the likelihood of having a fire. However, we need to investigate all the other variables, which is why we will plot a correlation matrix of the features in the dataset.

```{r}
corr_mat <- round(cor(df_scaled),2)
p_mat <- cor_pmat(df_scaled)
 
corr_mat <- ggcorrplot(
  corr_mat, 
  hc.order = FALSE, 
  type = "upper",
  outline.col = "white",
)
 
ggplotly(corr_mat)
```

### Feature Selection

We performed feature selection using the Caret package to determine which features are the most important and which are the least. 

In this case, we opted for Linear Discriminant Analysis with Stepwise Feature Selection by specifying *stepLDA* as our method.

The *varImp* function returns a measure of importance out of 100 for each of the features. According to the official [Caret documentation](https://topepo.github.io/caret/variable-importance.html), the importance metric is calculated by conducting a ROC curve analysis on each predictor; a series of cutoffs is applied to the predictor data to predict the class. The AUC is then computed and is used as a measure of variable importance. 


```{r}
# prepare training scheme
df_scaled$Classes = as.factor(df_scaled$Classes)

control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
modelLDA <- train(Classes~., data=df_scaled, method="stepLDA", trControl=control)

importanceLDA <- varImp(modelLDA, scale=FALSE)

plot(importanceLDA)
```
We can see that the variables *month*, *Ws*, *Region*, and *day* are insignificant compared to other features. We will disregard them in our model.



## Model Building

For the following models, we will only use the features that were the most significant in our feature selection phase. The selected features are:

1. Temperature
2. Rain
3. FFMC
4. DMC
5. DC
6. ISI
7. BUI
8. FWI
9. RH

### Splitting the dataset

We begin by splitting the data into train/test sets with a 80/20 split. This split was chosen by default as a good practice. This will leave us with 191 observations in the training set as well as 52 in the test set. Due to the small nature of the dataset at hand we will later apply cross validation in order to further examine the performance of our models and compare them with each other.


```{r}

set.seed(6)
split <- sample.split(df_scaled, SplitRatio=0.8)

train_set <- subset(df_scaled, split == "TRUE")
test_set <- subset(df_scaled, split=="FALSE")

dim(train_set)
dim(test_set)

```

### Logistic Regression

Logistic Regression is considered to be an extension of Linear Regression, in which we predict the qualitative response for an observation. It gives us the probability of a certain observation belonging to a class in binomial classification, but can also be extended to be used for multiple classifications.


#### Training the model

We first start by fitting our model on the training set. 

```{r}
logistic_model <- glm(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set, family="binomial")

logistic_model
```

[Interpretation on the coefficients]

#### Testing the model

Since logistic regression gives us the probability of each observation belonging to the 1 class, we will use a 0.5 threshold to transform that probability into a classification of either 0 or 1.

After getting our predictions, we will use the confusion matrix function from the caret library that computes a set of performance matrices including f1-score, recall and precision. Other matrices computed include: sensitivity, specificity, prevalence etc. The official documentation for this function and the formulas for all matrices are found in this [link.](https://rdrr.io/cran/caret/man/confusionMatrix.html) We will only be interested in the f1-score, recall, precision, accuracy and balanced accuracy.

##### On the train set


```{r}
preds_logistic <- predict(logistic_model, train_set, type="response")

preds_logistic <- ifelse(preds_logistic >0.5,1,0)
preds_logistic <- as.factor(preds_logistic)

confusionMatrix(preds_logistic, train_set$Classes,
                mode = "everything",
                positive="1")
```


##### On the test set

```{r}
preds_logistic <- predict(logistic_model, test_set, type="response")

preds_logistic <- ifelse(preds_logistic >0.5,1,0)
preds_logistic <- as.factor(preds_logistic)

confusionMatrix(preds_logistic, test_set$Classes,
                mode = "everything",
                positive="1")
```

#### Plotting the ROC curve

As we plot the ROC curve, we can see that the AUC is equal to 0.986 which is almost a perfect classifier. Due to the small size of the dataset we have at hand we cannot make any conclusions yet until we try out the different other statistical learning methods.

```{r}
ROCPred <- prediction(as.numeric(preds_logistic), test_set$Classes)

ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(ROCPer, colorize = TRUE)
```

#### Observations

Our statistics show the following:

1. On the training set: Accuracy is 100%, F1 score is 100%, with one False Negative and no False Positives.
2. On the testing set: Accuracy is 98%, F1 score is 98.6%, with one False Negative and no False Positives.

### LDA

Linear Discriminant Analysis is best used when the decision boundary of our given dataset is assumed to be linear. There are two basic assumptions that LDA takes into consideration:

1. There is a common variance across all response classes
2. The distribution of observations in each response class is normal with a **class-specific** mean, and a **common** variance


Since LDA assumes that each input variable has the same variance, we will use the standardized data-frame in the train test splits. Each variable in the standardized data-frame has mean of 0 and variance of 1.

#### Training the model


```{r}
lda_model = lda(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set, family="binomial")
lda_model
```


[Interpretation on the coefficients]

#### Testing the model

##### On the train set

On our trainng data, the model reached an accuracy of 94.2% and an F1 score of 94.4%. 

```{r}
preds_lda = predict(lda_model,train_set, type="response")
confusionMatrix(preds_lda$class, train_set$Classes,
                mode = "everything",
                positive="1")
```

##### On the test set

As we can see below, our the number of false positives is 0, and the number of false negatives is 1. The results are very good but the other way around would have been better as we do not want to miss any positives meaning we want to predict all fires. Our model yielded an f1-score of 98.6% and an accuracy of 98%.

```{r}
preds_lda = predict(lda_model,test_set, type="response")
confusionMatrix(preds_lda$class, test_set$Classes,
                mode = "everything",
                positive="1")
```
#### Potting the ROC curve

The AUC for LDA was also 0.986, similar to the one for Logistic Regression.

```{r}
ROCPred <- prediction(as.numeric(preds_lda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(ROCPer, colorize = TRUE)
```

[INTERPRETATION]

### QDA

Quadratic Discriminant Analysis is best used when the decision boundary of our given dataset is assumed to be non-linear. Similarly to LDA, QDA makes two basic assumptions: 

1. There is a different covariance for each of the response classes
2. The distribution of observations in each response class is normal with a **class-specific** mean, and a **class-specific** covariance


#### Training the model


```{r}
qda_model = qda(Classes ~ Temperature+Rain+FFMC+DMC+DC+ISI+BUI+FWI+RH, data=train_set)
qda_model
```

[Interpretation on the coefficients]

#### Testing the model

##### On the train set

Our model yields an accuracy of 97.9% and an F1 score of 98% on the training set.

```{r}
preds_qda = predict(qda_model,train_set, type="response")
confusionMatrix(preds$class, train_set$Classes,
                mode = "everything",
                positive="1")
```

##### On the test set

As we can see below, our the number of false positives is 0, and the number of false negatives is 2. The results are very good but the other way around would have been better as we do not want to miss any positives meaning we want to predict all fires. Our model yielded an f1-score of 96% and an accuracy of 96% as well.  

```{r}
preds_qda = predict(qda_model,test_set, type="response")
confusionMatrix(preds_qda$class, test_set$Classes,
                mode = "everything",
                positive="1")
```

#### Potting the ROC curve

After plotting the ROC curve we got an AUC of 0.953, which is a bit worse than the one in Logistic Regression and LDA.

```{r}

ROCPred <- prediction(as.numeric(preds_qda$class), test_set$Classes)
ROCPer <- performance(ROCPred, measure="tpr",x.measure="fpr")
auc <- performance(ROCPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(ROCPer, colorize = TRUE)
```




